import os
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

def scrape_teema():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # åˆå§‹åŒ–ç©ºæª”æ¡ˆï¼Œç¢ºä¿ä¸Šå‚³æ­¥é©Ÿä¸æœƒå¤±æ•—
    pd.DataFrame(columns=["å…¬å¸åç¨±"]).to_csv("teema_companies.csv", index=False, encoding="utf-8-sig")

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        url = "https://b2b.teema.org.tw/CompanyList.aspx"
        driver.get(url)
        wait = WebDriverWait(driver, 20)

        all_companies = []
        page_num = 1

        while True:
            print(f"ğŸš€ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é ...")
            
            # ç­‰å¾…å…¬å¸åç¨±æ¬„ä½å‡ºç¾
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[id*="hlCompanyName"]')))
            except TimeoutException:
                print("ç­‰å¾…è¶…æ™‚ï¼Œå¯èƒ½å·²åˆ°æœ€å¾Œä¸€é ã€‚")
                break
            
            # ç²å–ç•¶å‰é é¢å…¬å¸åç¨±
            items = driver.find_elements(By.CSS_SELECTOR, 'a[id*="hlCompanyName"]')
            for item in items:
                name = item.text.strip()
                if name:
                    all_companies.append({"å…¬å¸åç¨±": name})
            
            print(f"âœ… ç¬¬ {page_num} é æŠ“å–å®Œç•¢ï¼Œç›®å‰ç´¯è¨ˆ {len(all_companies)} ç­†è³‡æ–™")

            # å°‹æ‰¾ã€Œä¸‹ä¸€é ã€æŒ‰éˆ•
            # æ ¹æ“šå¸¸è¦‹çµæ§‹æœå°‹åŒ…å« ">" ç¬¦è™Ÿçš„é€£çµæˆ–ç‰¹å®š ID
            try:
                # å„ªå…ˆå°‹æ‰¾æ–‡å­—åŒ…å« ">" çš„æŒ‰éˆ•ï¼Œé€™é€šå¸¸æ˜¯åˆ†é åˆ—çš„ã€Œä¸‹ä¸€é ã€
                next_btn = driver.find_element(By.XPATH, "//a[contains(text(), '>')]")
                
                # æ»¾å‹•åˆ°æŒ‰éˆ•ä½ç½®ä¸¦é»æ“Š
                driver.execute_script("arguments[0].scrollIntoView();", next_btn)
                time.sleep(1)
                driver.execute_script("arguments[0].click();", next_btn)
                
                page_num += 1
                time.sleep(3) # ç·©è¡æ™‚é–“ï¼Œé¿å…è«‹æ±‚éå¿«
            except NoSuchElementException:
                print("ğŸ æ‰¾ä¸åˆ°ä¸‹ä¸€é æŒ‰éˆ•ï¼ŒæŠ“å–çµæŸã€‚")
                break

        # æœ€çµ‚å„²å­˜ä¸é‡è¤‡è³‡æ–™
        if all_companies:
            df = pd.DataFrame(all_companies).drop_duplicates()
            df.to_csv("teema_companies.csv", index=False, encoding="utf-8-sig")
            print(f"ğŸ‰ ä»»å‹™å¤§åŠŸå‘Šæˆï¼ç¸½è¨ˆæŠ“å– {len(df)} ç­†å…¬å¸åå–®ã€‚")

    except Exception as e:
        print(f"âŒ åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        if 'driver' in locals():
            driver.quit()

if __name__ == "__main__":
    scrape_teema()
