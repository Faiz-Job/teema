import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_teema():
    url = "https://b2b.teema.org.tw/CompanyList.aspx"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Referer": url
    }
    
    session = requests.Session()
    all_companies = []

    try:
        # 1. æŠ“å–ç¬¬ä¸€é 
        print("æ­£åœ¨æŠ“å–ç¬¬ 1 é ...")
        res = session.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')

        def extract_names(s):
            # æ ¹æ“šæ‚¨è§€å¯Ÿåˆ°çš„ ID çµæ§‹æŠ“å–å…¬å¸åç¨±
            links = s.select('a[id*="lnkCompanyName"], a[id*="hlCompanyName"]')
            return [{"å…¬å¸åç¨±": l.get_text(strip=True)} for l in links if l.get_text(strip=True)]

        all_companies.extend(extract_names(soup))

        # 2. æ¨¡æ“¬é»æ“Š ctl02, ctl03, ctl04, ctl05
        # é€™è£¡ i å°æ‡‰æ‚¨çœ‹åˆ°çš„ç·¨è™Ÿ
        for i in range(2, 6):
            target = f'ctl00$ContentPlaceHolder1$Repeater1$ctl0{i}$lnkPage'
            print(f"æ­£åœ¨æ¨¡æ“¬é»æ“Šåˆ†é æŒ‰éˆ•ï¼š{target}...")

            # æ¯æ¬¡ PostBack éƒ½è¦æ”œå¸¶æœ€æ–°çš„éš±è—æ¬„ä½å€¼
            payload = {
                "__EVENTTARGET": target,
                "__EVENTARGUMENT": "",
                "__VIEWSTATE": soup.find("input", {"name": "__VIEWSTATE"})["value"],
                "__VIEWSTATEGENERATOR": soup.find("input", {"name": "__VIEWSTATEGENERATOR"})["value"],
                "__EVENTVALIDATION": soup.find("input", {"name": "__EVENTVALIDATION"})["value"],
            }

            # å¿…é ˆä½¿ç”¨ POST æ–¹æ³•
            res = session.post(url, headers=headers, data=payload)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            p_data = extract_names(soup)
            if not p_data:
                print(f"è­¦å‘Šï¼šåœ¨ {target} æœªæŠ“åˆ°è³‡æ–™ã€‚")
                break
                
            all_companies.extend(p_data)
            print(f"æˆåŠŸæŠ“å–åˆ†é è³‡æ–™ï¼Œç´¯è¨ˆ {len(all_companies)} ç­†")
            time.sleep(2)

    except Exception as e:
        print(f"åŸ·è¡Œå‡ºéŒ¯: {e}")

    # å„²å­˜ CSV
    if all_companies:
        df = pd.DataFrame(all_companies).drop_duplicates()
        df.to_csv("teema_companies.csv", index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ ä»»å‹™å®Œæˆï¼å…±å­˜æª” {len(df)} ç­†è³‡æ–™ã€‚")

if __name__ == "__main__":
    scrape_teema()
